# 机器学习中问题随笔

## Word2Vec
[一文详解 Word2vec 之 Skip-Gram 模型](https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html)  
词向量就是用来将语言中的词进行数学化的一种方式，顾名思义，词向量就是把一个词表示成一个向量。

1. One-Hot Representation
用一个很长的向量来表示一个词，向量的长度为词典的大小，向量的分量只有一个1，其他全为0。1的位置对应该词在词典中的位置。
2. Distributed Representation
基本想法是直接用一个普通的向量表示一个词，这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, ...]，也就是普通的向量表示形式。维度以 50 维和 100 维比较常见。

由于是用向量表示，而且用较好的训练算法得到的词向量的向量一般是有空间上的意义的，也就是说，将所有这些向量放在一起形成一个词向量空间，而每一向量则为该空间中的一个点，在这个空间上的词向量之间的距离度量也可以表示对应的两个词之间的“距离”。所谓两个词之间的“距离”，就是这两个词之间的语法，语义之间的相似性。

## 统计模型
### TF-IDF
TF：词频  
IDF：逆文档频率，是一个词语`普遍重要性`的度量。某一特定词语的IDF，可以通过文档总数除以包含该词语的文档数目，再取lg得到(以10为底的对数)。  
举例：原子能应用

# 机器学习

## 逻辑回归

### 适用性

1. 用过概率预测：比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大；

2. 用于分类：设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类；

3. **仅能用于线性问题**：只有当目标和特征是线性关系时，才能用逻辑回归。

### 逻辑回归与朴素贝叶斯的区别

1. LR是判别模型，NB是生成模型；
2. LR是最大似然，NB是贝叶斯，概率哲学间的去呗；
3. LR要求目标特征间是线性关系，NB要求特征条件独立假设。

### 逻辑回归与线性回归的区别

1. 逻辑回归的输出只能为0、1，线性回归输出是连续值；
2. 逻辑回归在线性回归的拟合函数上加了一层逻辑函数；
3. 逻辑回归的参数计算方式为极大似然估计，而线性回归为最小二乘法；

> 最小二乘和极大似然估计可以相互替代吗？
>
> 不行。极大似然是计算使得数据出现的可能性最大的参数，依据是probability，最小二乘法是计算误差。

## [目标函数、损失函数、代价函数](https://www.zhihu.com/question/52398145/answer/209358209)

**损失函数(loss function)与代价函数(cost function)**：度量预测值与真实值之间的差异  

> 损失函数一般针对单个样本i，损失函数一般针对总体

**经验风险函数(risk function)**：是损失函数的期望，关于训练集的平均损失称作经验风险(enpirical risk)，我们的目标就是`经验风险最小化`  
**结构风险函数**：这个函数专门用来度量模型的复杂度，在机器学习中也叫正则化(regularization)。常用的有L1,L2范数  
**目标函数**：最优化经验风险和结构风险，这个函数就被称作目标函数  



### [局部加权线性回归](https://www.cnblogs.com/czdbest/p/5767138.html)
离x很近的样本，权值接近于1，而对于离x很远的样本，此时权值接近于0，这样就是在x局部构成线性回归，它依赖的也只是x周边的点  
局部加权回归在每一次预测新样本时都会重新确定参数，从而达到更好的预测效果。当数据规模较大的时候，计算量很大，学习效率很低。并且局部加权回归也不一定就是避免欠拟合(underfitting)  
**参数学习算法**：例如线性回归，一定训练出参数\theta，对于之后的预测不需要再使用原始训练数据集
**非参数学习算法**：每次进行预测都需要全部的训练数据，没有固定的参数\theta 



### [最优化算法](https://www.cnblogs.com/guoyaohua/p/8542554.html)
**BGD(batch gradient descent)梯度下降**  
采用整个训练集的数据来计算cost function对参数的梯度。
缺点：①针对整个数据集，计算成本大②不能投入新数据实时更新模型
效果：对凸函数可以收敛到全局极小值，对非凸函数收敛到局部极小值  
**SGD(stochastic gradient descent)随机梯度下降**  
对于很大的数据集，可能会有相似的样本，这样BGD在计算梯度时会出现冗余。SGD一次只进行一次更新，就没有冗余，而且比较快，可以新增样本。  
缺点：SGD的噪音较BGD多，并不是每次迭代都向着整体最优方向。  
**MBGD(mini-batch gradient descent)批量梯度下降**  
MBGD每一次利用一小批样本，可以降低参数更新时的方差，收敛更稳定。  
缺点：①不能保证很好的收敛性，learning rate如果选择的太小，收敛速度会很慢；如果太大，lost function就会在极小值处不停地震荡甚至偏离。（措施：先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小learning rate。~~不过这个阈值的设定需要提前写好，这样的话就不能适应数据集的特点。~~）②SGD对所有参数更新时应用同样的learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。learning rate会随着更新的次数逐渐变小。

### [激活函数](https://blog.csdn.net/tyhj_sf/article/details/79932893)
**常见激活函数**：Sigmoid函数、tanh函数、Relu函数，掌握其数学形式及几何图像，同时了解各激活函数的优缺点，具体见参考文献。  
**为什么需要激活函数**：如果不用激活函数（相当于激活函数是f(x)=x），在这种情况下，每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机，那么网络的逼近能力就相当有限。正因为上述原因，我们引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。  

### 正则化
目的：是使得模型参数的总和尽量小，达到的是参数稀疏化或者平滑的效果。最终的真实目标是抑制模型的过拟合。  
过拟合：打个不恰当的例子，如果模型参数太大了，就像有人偏科，那针对某类问题可以解决好，对其它问题效果会很差。参数较均衡，普适性会更好一些。


# 数据挖掘
## 十大算法系列
1. C4.5决策树
2. k-均值(K-mean)
3. 支持向量机(SVM)
4. Apriori
5. 最大期望算法(EM)
6. PageRank算法
7. AdaBoost算法
8. k-近邻算法(kNN)
9. 朴素贝叶斯算法(NB)
10. 分类回归树(CART)算法

